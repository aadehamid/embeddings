{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-13T23:44:48.513763Z",
     "start_time": "2024-02-13T23:44:46.713415Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamidadesokan/miniforge3/envs/wo_rag/lib/python3.10/site-packages/llama_index/download/module.py:12: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "import os\n",
    "# -----------------------------------------------------------------------------\n",
    "# openai and weaviate\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENA_AI_KEY\")\n",
    "import weaviate  \n",
    "WEAVIATE_URL = os.getenv(\"WEAVIATE_URL\")\n",
    "WEAVIATE_APIKEY = os.getenv(\"WEAVIATE_API_KEY\")\n",
    "from weaviate.auth import AuthApiKey\n",
    "# -----------------------------------------------------------------------------\n",
    "# import utilities\n",
    "from rag_utils import (\n",
    "    load_data_to_sql_db, \n",
    "    text_to_query_engine, \n",
    "    build_sentence_window_index_vector_DB,\n",
    "    setup_query_engines,\n",
    "    get_retry_guideline_response,\n",
    "    evaluate_and_transform_query,\n",
    "    build_sentence_window_index)\n",
    "# from RAG.WO_notebooks.rag_utils import build_sentence_window_index_vector_DB\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# llama_index imports\n",
    "#--------------------------------------------------------------------------------\n",
    "from llama_index.response.notebook_utils import display_response\n",
    "from llama_index import (\n",
    "    OpenAIEmbedding, \n",
    "    Document,)\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.vector_stores import WeaviateVectorStore\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo\"\n",
    "embedding_model_name = \"text-embedding-3-large\"\n",
    "# embedding_model_name=\"local:BAAI/bge-small-en-v1.5\"\n",
    "llm = OpenAI(temperature=0.1, model = model_name)\n",
    "embed_model = OpenAIEmbedding(model= embedding_model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T23:44:49.816266Z",
     "start_time": "2024-02-13T23:44:49.814256Z"
    }
   },
   "id": "8ad63c89d9ff18cc",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamidadesokan/miniforge3/envs/wo_rag/lib/python3.10/site-packages/weaviate/warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "auth_config = AuthApiKey(api_key=WEAVIATE_APIKEY)\n",
    "\n",
    "client = weaviate.Client(\n",
    "  url=WEAVIATE_URL,\n",
    "  auth_client_secret=auth_config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T23:44:50.585323Z",
     "start_time": "2024-02-13T23:44:50.288990Z"
    }
   },
   "id": "8a6e26ea9d76a575",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Connect to a WCS instance\n",
    "# client = weaviate.connect_to_wcs(\n",
    "#     cluster_url=WEAVIATE_URL,\n",
    "#     auth_credentials=weaviate.auth.AuthApiKey(WEAVIATE_APIKEY))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T16:17:19.279895Z",
     "start_time": "2024-02-13T16:17:19.275977Z"
    }
   },
   "id": "811232292629c5e5",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Work Order Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b4fb182cb554eb3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# path to the raw dataset\n",
    "table_name = \"work_order_table\"\n",
    "dbpath = \"./data/wo_data.db\"\n",
    "wo_data_path=\"/Users/hamidadesokan/Dropbox/2_Skill_Development/DLML/genai_applications/embeddings/RAG/WO_notebooks/data/excavator_2015_cleaned_forpdl.csv\"\n",
    "conn, engine, data = load_data_to_sql_db(wo_data_path, dbpath, table_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T23:44:54.168532Z",
     "start_time": "2024-02-13T23:44:54.124739Z"
    }
   },
   "id": "4cb7b7da74a87bb0",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# test the query engine"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f7b0892861da2b6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "**`Final Response:`** In 2011, we spent a total of $1,534,557.32."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_name = \"gpt-3.5-turbo\"\n",
    "# embedding_model_name = \"text-embedding-3-large\"\n",
    "# llm = OpenAI(temperature=0.1, model = model_name)\n",
    "# embed_model = OpenAIEmbedding(model= embedding_model_name)\n",
    "# query_engine,sql_database, service_context = text_to_query_engine(model_name, embedding_model_name, table_name, engine)\n",
    "# # query_str = \"Which work oder cost 183.05?\"\n",
    "# query_str = \"How much in total did we spend in 2011?\"\n",
    "# response = query_engine.query(query_str)\n",
    "# display_response(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T21:39:54.133913Z",
     "start_time": "2024-02-13T21:39:49.713400Z"
    }
   },
   "id": "3459ee0175fd8e27",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create document object"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8bb83d2c44c9ecb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "columns_to_embed = [\"OriginalShorttext\", \n",
    "                \"MajorSystem\" , \"Part\",\n",
    "                \"Action\", \"FM\", \"Location\", \"FuncLocation\"]\n",
    "\n",
    "columns_to_metadata = ['Asset',  'Cost', 'RunningTime', 'Variant', 'Comments', 'SuspSugg', 'Rule']\n",
    "\n",
    "\n",
    "docs = []\n",
    "for i, row in data.iterrows():\n",
    "    to_metadata = {col: row[col] for col in columns_to_metadata if col in row}\n",
    "    values_to_embed = {k: str(row[k]) for k in columns_to_embed if k in row}\n",
    "    to_embed = '\\n'.join(f\"{k.strip()}: {v.strip()}\" for k, v in values_to_embed.items())\n",
    "    newDoc = Document(text=to_embed, metadata=to_metadata)\n",
    "    docs.append(newDoc)\n",
    "\n",
    "# Create a single document from a list of Documents\n",
    "# this is what we will chunk up and store with its embedding in Weaviate\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in docs]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T23:45:00.962690Z",
     "start_time": "2024-02-13T23:45:00.615874Z"
    }
   },
   "id": "22fa738a472a35ac",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build Sentence window"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c2dead8534df91"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product schema was created.\n"
     ]
    }
   ],
   "source": [
    "# load work_order_schema from json\n",
    "client.schema.delete_class(\"WorkOrder\")\n",
    "import json\n",
    "with open(\"./config/work_order_schema\", \"r\") as f:\n",
    "    work_order_schema = json.load(f)\n",
    "\n",
    "client.schema.create(work_order_schema)\n",
    "print(\"Product schema was created.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T23:45:06.245224Z",
     "start_time": "2024-02-13T23:45:05.876496Z"
    }
   },
   "id": "698bc457b39e9db0",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamidadesokan/miniforge3/envs/wo_rag/lib/python3.10/site-packages/weaviate/__init__.py:128: DeprecationWarning: Dep010: Importing AuthApiKey from weaviate is deprecated. Please import it from its specific module: weaviate.auth\n",
      "  _Warnings.root_module_import(name, map_[name])\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 8260 tokens (8260 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mBadRequestError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m work_order_index \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_sentence_window_index_vector_DB\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdocument\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mdocument\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclient\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mclient\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mllm\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mllm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43membed_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membed_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprefix\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mWork_order_sent_win_index\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m      7\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Dropbox/2_Skill_Development/DLML/genai_applications/embeddings/RAG/WO_notebooks/rag_utils.py:410\u001B[0m, in \u001B[0;36mbuild_sentence_window_index_vector_DB\u001B[0;34m(document, client, llm, embed_model, save_dir, prefix)\u001B[0m\n\u001B[1;32m    407\u001B[0m vector_store \u001B[38;5;241m=\u001B[39m WeaviateVectorStore(weaviate_client\u001B[38;5;241m=\u001B[39mclient, class_prefix\u001B[38;5;241m=\u001B[39mprefix)\n\u001B[1;32m    408\u001B[0m storage_context \u001B[38;5;241m=\u001B[39m StorageContext\u001B[38;5;241m.\u001B[39mfrom_defaults(vector_store\u001B[38;5;241m=\u001B[39mvector_store)\n\u001B[0;32m--> 410\u001B[0m sentence_index \u001B[38;5;241m=\u001B[39m \u001B[43mVectorStoreIndex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocument\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_context\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_context\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    411\u001B[0m \u001B[43m                                                 \u001B[49m\u001B[43mservice_context\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msentence_context\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    413\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sentence_index\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/site-packages/llama_index/indices/base.py:110\u001B[0m, in \u001B[0;36mBaseIndex.from_documents\u001B[0;34m(cls, documents, storage_context, service_context, show_progress, **kwargs)\u001B[0m\n\u001B[1;32m    101\u001B[0m     docstore\u001B[38;5;241m.\u001B[39mset_document_hash(doc\u001B[38;5;241m.\u001B[39mget_doc_id(), doc\u001B[38;5;241m.\u001B[39mhash)\n\u001B[1;32m    103\u001B[0m nodes \u001B[38;5;241m=\u001B[39m run_transformations(\n\u001B[1;32m    104\u001B[0m     documents,  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    105\u001B[0m     service_context\u001B[38;5;241m.\u001B[39mtransformations,\n\u001B[1;32m    106\u001B[0m     show_progress\u001B[38;5;241m=\u001B[39mshow_progress,\n\u001B[1;32m    107\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    108\u001B[0m )\n\u001B[0;32m--> 110\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    111\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    112\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_context\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_context\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    113\u001B[0m \u001B[43m    \u001B[49m\u001B[43mservice_context\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mservice_context\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    114\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshow_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshow_progress\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    115\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    116\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/site-packages/llama_index/indices/vector_store/base.py:53\u001B[0m, in \u001B[0;36mVectorStoreIndex.__init__\u001B[0;34m(self, nodes, objects, index_struct, service_context, storage_context, use_async, store_nodes_override, insert_batch_size, show_progress, **kwargs)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_store_nodes_override \u001B[38;5;241m=\u001B[39m store_nodes_override\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_insert_batch_size \u001B[38;5;241m=\u001B[39m insert_batch_size\n\u001B[0;32m---> 53\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindex_struct\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex_struct\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m    \u001B[49m\u001B[43mservice_context\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mservice_context\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_context\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_context\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshow_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshow_progress\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[43m    \u001B[49m\u001B[43mobjects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobjects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     61\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/site-packages/llama_index/indices/base.py:73\u001B[0m, in \u001B[0;36mBaseIndex.__init__\u001B[0;34m(self, nodes, objects, index_struct, storage_context, service_context, show_progress, **kwargs)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m index_struct \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     nodes \u001B[38;5;241m=\u001B[39m nodes \u001B[38;5;129;01mor\u001B[39;00m []\n\u001B[0;32m---> 73\u001B[0m     index_struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild_index_from_nodes\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnodes\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mobjects\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore\u001B[39;49;00m\n\u001B[1;32m     75\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index_struct \u001B[38;5;241m=\u001B[39m index_struct\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_storage_context\u001B[38;5;241m.\u001B[39mindex_store\u001B[38;5;241m.\u001B[39madd_index_struct(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index_struct)\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/site-packages/llama_index/indices/vector_store/base.py:274\u001B[0m, in \u001B[0;36mVectorStoreIndex.build_index_from_nodes\u001B[0;34m(self, nodes, **insert_kwargs)\u001B[0m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(\n\u001B[1;32m    267\u001B[0m     node\u001B[38;5;241m.\u001B[39mget_content(metadata_mode\u001B[38;5;241m=\u001B[39mMetadataMode\u001B[38;5;241m.\u001B[39mEMBED) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m nodes\n\u001B[1;32m    268\u001B[0m ):\n\u001B[1;32m    269\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    270\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot build index from nodes with no content. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    271\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease ensure all nodes have content.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    272\u001B[0m     )\n\u001B[0;32m--> 274\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_build_index_from_nodes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minsert_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/site-packages/llama_index/indices/vector_store/base.py:246\u001B[0m, in \u001B[0;36mVectorStoreIndex._build_index_from_nodes\u001B[0;34m(self, nodes, **insert_kwargs)\u001B[0m\n\u001B[1;32m    244\u001B[0m     run_async_tasks(tasks)\n\u001B[1;32m    245\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 246\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_add_nodes_to_index\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    247\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex_struct\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    248\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    249\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshow_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_show_progress\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    250\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minsert_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    251\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m index_struct\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/site-packages/llama_index/indices/vector_store/base.py:199\u001B[0m, in \u001B[0;36mVectorStoreIndex._add_nodes_to_index\u001B[0;34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001B[0m\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m nodes_batch \u001B[38;5;129;01min\u001B[39;00m iter_batch(nodes, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_insert_batch_size):\n\u001B[0;32m--> 199\u001B[0m     nodes_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_node_with_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnodes_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshow_progress\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    200\u001B[0m     new_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_vector_store\u001B[38;5;241m.\u001B[39madd(nodes_batch, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minsert_kwargs)\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_vector_store\u001B[38;5;241m.\u001B[39mstores_text \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_store_nodes_override:\n\u001B[1;32m    203\u001B[0m         \u001B[38;5;66;03m# NOTE: if the vector store doesn't store text,\u001B[39;00m\n\u001B[1;32m    204\u001B[0m         \u001B[38;5;66;03m# we need to add the nodes to the index struct and document store\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/site-packages/llama_index/indices/vector_store/base.py:107\u001B[0m, in \u001B[0;36mVectorStoreIndex._get_node_with_embedding\u001B[0;34m(self, nodes, show_progress)\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_node_with_embedding\u001B[39m(\n\u001B[1;32m     97\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     98\u001B[0m     nodes: Sequence[BaseNode],\n\u001B[1;32m     99\u001B[0m     show_progress: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    100\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[BaseNode]:\n\u001B[1;32m    101\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Get tuples of id, node, and embedding.\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \n\u001B[1;32m    103\u001B[0m \u001B[38;5;124;03m    Allows us to store these nodes in a vector store.\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m    Embeddings are called in batches.\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \n\u001B[1;32m    106\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 107\u001B[0m     id_to_embed_map \u001B[38;5;241m=\u001B[39m \u001B[43membed_nodes\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_service_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshow_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshow_progress\u001B[49m\n\u001B[1;32m    109\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    111\u001B[0m     results \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m nodes:\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/site-packages/llama_index/indices/utils.py:137\u001B[0m, in \u001B[0;36membed_nodes\u001B[0;34m(nodes, embed_model, show_progress)\u001B[0m\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    135\u001B[0m         id_to_embed_map[node\u001B[38;5;241m.\u001B[39mnode_id] \u001B[38;5;241m=\u001B[39m node\u001B[38;5;241m.\u001B[39membedding\n\u001B[0;32m--> 137\u001B[0m new_embeddings \u001B[38;5;241m=\u001B[39m \u001B[43membed_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_text_embedding_batch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    138\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtexts_to_embed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshow_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshow_progress\u001B[49m\n\u001B[1;32m    139\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m new_id, text_embedding \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(ids_to_embed, new_embeddings):\n\u001B[1;32m    142\u001B[0m     id_to_embed_map[new_id] \u001B[38;5;241m=\u001B[39m text_embedding\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/site-packages/llama_index/core/embeddings/base.py:256\u001B[0m, in \u001B[0;36mBaseEmbedding.get_text_embedding_batch\u001B[0;34m(self, texts, show_progress, **kwargs)\u001B[0m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m idx \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(texts) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(cur_batch) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_batch_size:\n\u001B[1;32m    251\u001B[0m     \u001B[38;5;66;03m# flush\u001B[39;00m\n\u001B[1;32m    252\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mevent(\n\u001B[1;32m    253\u001B[0m         CBEventType\u001B[38;5;241m.\u001B[39mEMBEDDING,\n\u001B[1;32m    254\u001B[0m         payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mSERIALIZED: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_dict()},\n\u001B[1;32m    255\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m event:\n\u001B[0;32m--> 256\u001B[0m         embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_text_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcur_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    257\u001B[0m         result_embeddings\u001B[38;5;241m.\u001B[39mextend(embeddings)\n\u001B[1;32m    258\u001B[0m         event\u001B[38;5;241m.\u001B[39mon_end(\n\u001B[1;32m    259\u001B[0m             payload\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m    260\u001B[0m                 EventPayload\u001B[38;5;241m.\u001B[39mCHUNKS: cur_batch,\n\u001B[1;32m    261\u001B[0m                 EventPayload\u001B[38;5;241m.\u001B[39mEMBEDDINGS: embeddings,\n\u001B[1;32m    262\u001B[0m             },\n\u001B[1;32m    263\u001B[0m         )\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/site-packages/llama_index/embeddings/openai.py:413\u001B[0m, in \u001B[0;36mOpenAIEmbedding._get_text_embeddings\u001B[0;34m(self, texts)\u001B[0m\n\u001B[1;32m    406\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Get text embeddings.\u001B[39;00m\n\u001B[1;32m    407\u001B[0m \n\u001B[1;32m    408\u001B[0m \u001B[38;5;124;03mBy default, this is a wrapper around _get_text_embedding.\u001B[39;00m\n\u001B[1;32m    409\u001B[0m \u001B[38;5;124;03mCan be overridden for batch queries.\u001B[39;00m\n\u001B[1;32m    410\u001B[0m \n\u001B[1;32m    411\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    412\u001B[0m client \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_client()\n\u001B[0;32m--> 413\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mget_embeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    414\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclient\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[43m    \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_text_engine\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madditional_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    418\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/site-packages/tenacity/__init__.py:289\u001B[0m, in \u001B[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001B[0;34m(*args, **kw)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(f)\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped_f\u001B[39m(\u001B[38;5;241m*\u001B[39margs: t\u001B[38;5;241m.\u001B[39mAny, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: t\u001B[38;5;241m.\u001B[39mAny) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m t\u001B[38;5;241m.\u001B[39mAny:\n\u001B[0;32m--> 289\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/site-packages/tenacity/__init__.py:379\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    377\u001B[0m retry_state \u001B[38;5;241m=\u001B[39m RetryCallState(retry_object\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, fn\u001B[38;5;241m=\u001B[39mfn, args\u001B[38;5;241m=\u001B[39margs, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m    378\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 379\u001B[0m     do \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    380\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[1;32m    381\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/site-packages/tenacity/__init__.py:325\u001B[0m, in \u001B[0;36mBaseRetrying.iter\u001B[0;34m(self, retry_state)\u001B[0m\n\u001B[1;32m    323\u001B[0m     retry_exc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretry_error_cls(fut)\n\u001B[1;32m    324\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreraise:\n\u001B[0;32m--> 325\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[43mretry_exc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m retry_exc \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfut\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexception\u001B[39;00m()\n\u001B[1;32m    328\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwait:\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/site-packages/tenacity/__init__.py:158\u001B[0m, in \u001B[0;36mRetryError.reraise\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreraise\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m t\u001B[38;5;241m.\u001B[39mNoReturn:\n\u001B[1;32m    157\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_attempt\u001B[38;5;241m.\u001B[39mfailed:\n\u001B[0;32m--> 158\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlast_attempt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    159\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/concurrent/futures/_base.py:451\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    449\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[1;32m    450\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[0;32m--> 451\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    453\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_condition\u001B[38;5;241m.\u001B[39mwait(timeout)\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/concurrent/futures/_base.py:403\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 403\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[1;32m    404\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    405\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[1;32m    406\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/site-packages/tenacity/__init__.py:382\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[1;32m    381\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 382\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:  \u001B[38;5;66;03m# noqa: B902\u001B[39;00m\n\u001B[1;32m    384\u001B[0m         retry_state\u001B[38;5;241m.\u001B[39mset_exception(sys\u001B[38;5;241m.\u001B[39mexc_info())  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/site-packages/llama_index/embeddings/openai.py:178\u001B[0m, in \u001B[0;36mget_embeddings\u001B[0;34m(client, list_of_text, engine, **kwargs)\u001B[0m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(list_of_text) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2048\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe batch size should not be larger than 2048.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    176\u001B[0m list_of_text \u001B[38;5;241m=\u001B[39m [text\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m list_of_text]\n\u001B[0;32m--> 178\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlist_of_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdata\n\u001B[1;32m    179\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [d\u001B[38;5;241m.\u001B[39membedding \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m data]\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/site-packages/openai/resources/embeddings.py:113\u001B[0m, in \u001B[0;36mEmbeddings.create\u001B[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    107\u001B[0m         embedding\u001B[38;5;241m.\u001B[39membedding \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfrombuffer(  \u001B[38;5;66;03m# type: ignore[no-untyped-call]\u001B[39;00m\n\u001B[1;32m    108\u001B[0m             base64\u001B[38;5;241m.\u001B[39mb64decode(data), dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfloat32\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    109\u001B[0m         )\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[0;32m--> 113\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    114\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/embeddings\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    115\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEmbeddingCreateParams\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    116\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    117\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    121\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpost_parser\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    123\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCreateEmbeddingResponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    124\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/site-packages/openai/_base_client.py:1200\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[1;32m   1187\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1188\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1195\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1196\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m   1197\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1198\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1199\u001B[0m     )\n\u001B[0;32m-> 1200\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/site-packages/openai/_base_client.py:889\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    880\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[1;32m    881\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    882\u001B[0m     cast_to: Type[ResponseT],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    887\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    888\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m--> 889\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    891\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    892\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    893\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    894\u001B[0m \u001B[43m        \u001B[49m\u001B[43mremaining_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremaining_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    895\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/wo_rag/lib/python3.10/site-packages/openai/_base_client.py:980\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    977\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m    979\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 980\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    982\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[1;32m    983\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[1;32m    984\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    987\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m    988\u001B[0m )\n",
      "\u001B[0;31mBadRequestError\u001B[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 8260 tokens (8260 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "work_order_index = build_sentence_window_index_vector_DB(\n",
    "    document = [document],\n",
    "    client = client,\n",
    "    llm = llm,\n",
    "    embed_model=embed_model,\n",
    "    prefix = \"Work_order_sent_win_index\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T23:46:38.020922Z",
     "start_time": "2024-02-13T23:46:18.884307Z"
    }
   },
   "id": "e355cada3b5fa07a",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save the index in vectore store"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b13b08c4f0f4fd62"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "query_router_engine = setup_query_engines(sql_database, work_order_index, table_name)\n",
    "response = get_retry_guideline_response(query_router_engine, \"How much in total did we spend in 2011?\", guideline = False)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T21:41:33.495483Z",
     "start_time": "2024-02-13T21:41:31.043324Z"
    }
   },
   "id": "980919737d6fa6",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<llama_index.query_engine.router_query_engine.RouterQueryEngine at 0x2b8f2b280>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_router_engine"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T21:41:35.394295Z",
     "start_time": "2024-02-13T21:41:35.389574Z"
    }
   },
   "id": "fe4dffa009ab4a94",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2011, we spent a total of $1,534,557.32.\n"
     ]
    }
   ],
   "source": [
    "# query_str = \"Which work oder cost 183.05?\"\n",
    "query_str = \"How much in total did we spend in 2011?\"\n",
    "response = query_router_engine.query(query_str)\n",
    "print(str(response))\n",
    "# display_response(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T21:41:46.410342Z",
     "start_time": "2024-02-13T21:41:42.937690Z"
    }
   },
   "id": "e477fafe31815930",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "1534557.3200000003"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data.BscStartDate.str.startswith(\"2011\")].Cost.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T16:41:58.492987Z",
     "start_time": "2024-02-13T16:41:58.489206Z"
    }
   },
   "id": "98dc1e1268e21632",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f92073f4a9c2911c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluating query and prompts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83e1f405b6340536"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guideline eval evaluation result: The response fully answers the query by providing the specific amount spent in 2011, which is $1,534,557.32. It is not vague or ambiguous and uses statistics to provide a precise answer. The response is not overly long and effectively summarizes the information. Overall, it meets all the guidelines.\n",
      "Transformed query: Here is a previous bad answer.\n",
      "In 2011, we spent a total of $1,534,557.32.\n",
      "Here is some feedback from the evaluator about the response given.\n",
      "The response fully answers the query by providing the specific amount spent in 2011, which is $1,534,557.32. It is not vague or ambiguous and uses statistics to provide a precise answer. The response is not overly long and effectively summarizes the information. Overall, it meets all the guidelines.\n",
      "Now answer the question.\n",
      "How much money did we spend in total during the year 2011?\n"
     ]
    }
   ],
   "source": [
    "from llama_index.evaluation.guideline import DEFAULT_GUIDELINES\n",
    "from llama_index.evaluation import GuidelineEvaluator\n",
    "# Evaluating query and prompts\n",
    "# from llama_index.evaluation.guideline import DEFAULT_GUIDELINES\n",
    "# from llama_index.evaluation import GuidelineEvaluator\n",
    "from llama_index.response.schema import Response\n",
    "from llama_index.indices.query.query_transform.feedback_transform import (\n",
    "    FeedbackQueryTransformation,)\n",
    "\n",
    "# Guideline eval\n",
    "guideline_eval = GuidelineEvaluator(\n",
    "    guidelines=DEFAULT_GUIDELINES\n",
    "    + \"\\nThe response should not be overly long.\\n\"\n",
    "    \"The response should try to summarize where possible.\\n\"\n",
    "    \"First, answer the question\\n\"\n",
    "    \"Second provide the reason, why you choose that answer.\\n\"\n",
    ")  # just for example\n",
    "\n",
    "typed_response = (\n",
    "    retry_guideline_response if isinstance(retry_guideline_response, Response) else retry_guideline_response.get_response()\n",
    ")\n",
    "eval = guideline_eval.evaluate_response(query_str, typed_response)\n",
    "print(f\"Guideline eval evaluation result: {eval.feedback}\")\n",
    "\n",
    "feedback_query_transform = FeedbackQueryTransformation(resynthesize_query=True)\n",
    "transformed_query = feedback_query_transform.run(query_str, {\"evaluation\": eval})\n",
    "print(f\"Transformed query: {transformed_query.query_str}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T17:05:38.975604Z",
     "start_time": "2024-02-13T17:05:36.616219Z"
    }
   },
   "id": "419a9bf46cd00fef",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from typing import Tuple, Any\n",
    "from llama_index.response.schema import Response\n",
    "from llama_index.indices.query.query_transform.feedback_transform import FeedbackQueryTransformation\n",
    "from llama_index.evaluation.guideline import DEFAULT_GUIDELINES\n",
    "from llama_index.evaluation import GuidelineEvaluator\n",
    "\n",
    "def evaluate_and_transform_query(\n",
    "    query_str: str,\n",
    "    retry_response: Response,  # Type hint as `Any` since it could be a `Response` or another type with a `.get_response()` method.\n",
    "    DEFAULT_GUIDELINES: str\n",
    ") -> Tuple[GuidelineEvaluator, FeedbackQueryTransformation]:\n",
    "    \"\"\"\n",
    "    Evaluates a query response against a set of guidelines and transforms the query based on the feedback.\n",
    "\n",
    "    Args:\n",
    "        query_str (str): The original query string.\n",
    "        retry_guideline_response (Any): The initial response to evaluate, can be a `Response` object or another type with a `get_response()` method.\n",
    "        DEFAULT_GUIDELINES (str): The default guidelines string to be used for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str]: A tuple containing the guideline evaluation feedback and the transformed query string.\n",
    "    \"\"\"\n",
    "    # Initialize the guideline evaluator with additional guidelines.\n",
    "    guideline_eval = GuidelineEvaluator(\n",
    "        guidelines=DEFAULT_GUIDELINES\n",
    "        + \"\\nThe response should not be overly long.\\n\"\n",
    "          \"The response should try to summarize where possible.\\n\"\n",
    "          \"First, answer the question\\n\"\n",
    "          \"Second provide the reason, why you choose that answer.\\n\"\n",
    "    )\n",
    "\n",
    "    # Get the typed response based on the type of `retry_guideline_response`.\n",
    "    typed_response = (\n",
    "        retry_response if isinstance(retry_response, Response) else retry_response.get_response()\n",
    "    )\n",
    "\n",
    "    # Evaluate the response against the guidelines.\n",
    "    eval = guideline_eval.evaluate_response(query_str, typed_response)\n",
    "    print(f\"Guideline eval evaluation result: {eval.feedback}\")\n",
    "\n",
    "    # Transform the query based on feedback.\n",
    "    feedback_query_transform = FeedbackQueryTransformation(resynthesize_query=True)\n",
    "    transformed_query = feedback_query_transform.run(query_str, {\"evaluation\": eval})\n",
    "    print(f\"Transformed query: {transformed_query.query_str}\")\n",
    "\n",
    "    # Return the feedback and the transformed query string.\n",
    "    return eval.feedback, transformed_query.query_str\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T21:51:49.591834Z",
     "start_time": "2024-02-13T21:51:49.587985Z"
    }
   },
   "id": "29d77af4b4924877",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guideline eval evaluation result: The response fully answers the query by providing the specific amount spent in 2011, which is $1,534,557.32. It is not vague or ambiguous and uses statistics to provide a clear answer. The response is not overly long and summarizes the information effectively. Overall, it meets all the guidelines.\n",
      "Transformed query: Here is a previous bad answer.\n",
      "In 2011, we spent a total of $1,534,557.32.\n",
      "Here is some feedback from the evaluator about the response given.\n",
      "The response fully answers the query by providing the specific amount spent in 2011, which is $1,534,557.32. It is not vague or ambiguous and uses statistics to provide a clear answer. The response is not overly long and summarizes the information effectively. Overall, it meets all the guidelines.\n",
      "Now answer the question.\n",
      "How much money did we spend in total during the year 2011?\n"
     ]
    }
   ],
   "source": [
    "feedback, transformed_query = evaluate_and_transform_query(query_str, response, DEFAULT_GUIDELINES)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T21:51:53.728091Z",
     "start_time": "2024-02-13T21:51:51.881883Z"
    }
   },
   "id": "6fe420feee0f17c3",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a previous bad answer.\n",
      "In 2011, we spent a total of $1,534,557.32.\n",
      "Here is some feedback from the evaluator about the response given.\n",
      "The response fully answers the query by providing the specific amount spent in 2011, which is $1,534,557.32. It is not vague or ambiguous and uses statistics to provide a clear answer. The response is not overly long and summarizes the information effectively. Overall, it meets all the guidelines.\n",
      "Now answer the question.\n",
      "How much money did we spend in total during the year 2011?\n"
     ]
    }
   ],
   "source": [
    "print(transformed_query)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T21:52:19.159545Z",
     "start_time": "2024-02-13T21:52:19.156576Z"
    }
   },
   "id": "9910f40fc486195",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response fully answers the query by providing the specific amount spent in 2011, which is $1,534,557.32. It is not vague or ambiguous and uses statistics to provide a clear answer. The response is not overly long and summarizes the information effectively. Overall, it meets all the guidelines.\n"
     ]
    }
   ],
   "source": [
    "print(feedback)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T21:52:31.885411Z",
     "start_time": "2024-02-13T21:52:31.880325Z"
    }
   },
   "id": "84e10f21a8b3401b",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9dbb2a2ff574022e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
