{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rich import print\n",
    "from icecream import ic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>.<span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>.<span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m. \u001b[1;36m2\u001b[0m. \u001b[1;36m3\u001b[0m. \u001b[1;36m4\u001b[0m.\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m. \u001b[1;36m3\u001b[0m. \u001b[1;36m4\u001b[0m. \u001b[1;36m5\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"Hello, world\"\n",
    "# text_embedding = np.random.randint(1, 10, size=(2,4)) # random embedding for the two words\n",
    "text_embedding = np.array([[1, 2, 3, 4], [2, 3, 4, 5]], dtype=\"float\")\n",
    "print(text_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text embedding has no concept of order or position. Order of words matter in Englisah language at least. \n",
    "The position of each embedding vector is typically represented as another vector of the same size as the embedding vector. This means, we can also learn the position embedding or just use a fixed vector as they did in the landmark paper \"Attention is all you need\". The original paper used sine function foe even positions and cosine function for odd positions.\n",
    "\n",
    "$$ PE(pos, 2i) = sin(\\frac{pos}{1000^{\\frac{2i}{dmodel}}})$$\n",
    "$$ PE(pos, 2i+1) = cos(\\frac{pos}{1000^{\\frac{2i}{dmodel}}})$$\n",
    "\n",
    "- *dmodel* is the length of the embedding vector, which is 4 in this case. This is the number of columns in our embedding matrix.\n",
    "- *i* is the index of the elements of the embedding vectors\n",
    "- *pos* is the row number of the text. In our case, we have two heads and two rows.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def position_embedding(embed_vectors):\n",
    "    position, dmodel = embed_vectors.shape\n",
    "    pos_vec = np.empty_like(embed_vectors)\n",
    "    for pos in range(position):\n",
    "        for ind in range(dmodel):\n",
    "            if ind % 2 == 0:\n",
    "                pos_vec[pos, ind] = math.sin(pos / (1000 ** (2 * ind / dmodel)))\n",
    "            else:\n",
    "                pos_vec[pos, ind] = math.cos(pos / (1000 ** (2 * ind / dmodel)))\n",
    "    return pos_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00000000e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.00000000e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00000000e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.00000000e+00</span><span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.41470985e-01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.99500042e-01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.99999833e-04</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.99999999e-01</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0.00000000e+00\u001b[0m \u001b[1;36m1.00000000e+00\u001b[0m \u001b[1;36m0.00000000e+00\u001b[0m \u001b[1;36m1.00000000e+00\u001b[0m\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m8.41470985e-01\u001b[0m \u001b[1;36m9.99500042e-01\u001b[0m \u001b[1;36m9.99999833e-04\u001b[0m \u001b[1;36m9.99999999e-01\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_position = position_embedding(text_embedding)\n",
    "print(text_position)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>.         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>.         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>.        <span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.84147098</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.99950004</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.001</span>      <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>.        <span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.         \u001b[1;36m3\u001b[0m.         \u001b[1;36m3\u001b[0m.         \u001b[1;36m5\u001b[0m.        \u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m2.84147098\u001b[0m \u001b[1;36m3.99950004\u001b[0m \u001b[1;36m4.001\u001b[0m      \u001b[1;36m6\u001b[0m.        \u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add the text_embedding and text_position vector to create the\n",
    "# final encoder input embedding\n",
    "\n",
    "encoder_input = text_embedding + text_position\n",
    "print(encoder_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "The model use self attanetion to attend to different part of the encider input. Multi-head attention is a mechanism for the model to jointly attend to different parts of the input in different vector subsapces.\n",
    "This is accomplish with multiple attending heads each with its own K, V, and Q matrices.\n",
    "\n",
    "**Where did these matrices come from?**\n",
    "\n",
    "Let us assume we have two heads in this case. We then have six Q, K, and V matrices. The first three matrices belings to the first head and the last three matrices belong to the second head.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "WK1 = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 1], [0, 1, 0]])\n",
    "WV1 = np.array([[0, 1, 1], [1, 0, 0], [1, 0, 1], [0, 1, 0]])\n",
    "WQ1 = np.array([[0, 0, 0], [1, 1, 0], [0, 0, 1], [1, 0, 0]]) # 4 X 3\n",
    "WK2 = np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0], [0, 1, 0]])\n",
    "WV2 = np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1], [1, 0, 0]])\n",
    "WQ2 = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_QKV(embed, WK, WV, WQ):\n",
    "    embed_WK = np.dot(embed, WK)\n",
    "    embed_WV = np.dot(embed, WV)\n",
    "    embed_WQ = np.dot(embed, WQ)\n",
    "    return embed_WK, embed_WV, embed_WQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "K1, V1, Q1 = get_QKV(encoder_input, WK1, WV1, WQ1) # 2 X 3\n",
    "K2, V2, Q2 = get_QKV(encoder_input, WK2, WV2, WQ2) # 2 X 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>.         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>.         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>.        <span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.99950004</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.99950004</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.001</span>     <span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m.         \u001b[1;36m3\u001b[0m.         \u001b[1;36m3\u001b[0m.        \u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m9.99950004\u001b[0m \u001b[1;36m3.99950004\u001b[0m \u001b[1;36m4.001\u001b[0m     \u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(Q1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the dmodel reduced in size from 4 to 3 after we multiply the encoder input with those three matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/ np.sum(np.exp(x), axis = 1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(Q, K, V):\n",
    "    scores1 = np.dot(Q, K.T)\n",
    "    scores1 = scores1 / np.sqrt(K1.shape[1])\n",
    "    scores1 = softmax(scores1)\n",
    "    attention = scores1 @ V1\n",
    "    return scores1, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K1.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.52899472e-10</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.00000000e+00</span><span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.03843074e-12</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.00000000e+00</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m4.52899472e-10\u001b[0m \u001b[1;36m1.00000000e+00\u001b[0m\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m1.03843074e-12\u001b[0m \u001b[1;36m1.00000000e+00\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.00050004</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.84147098</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.84247098</span><span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.00050004</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.84147098</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.84247098</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8.00050004\u001b[0m \u001b[1;36m8.84147098\u001b[0m \u001b[1;36m6.84247098\u001b[0m\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m8.00050004\u001b[0m \u001b[1;36m8.84147098\u001b[0m \u001b[1;36m6.84247098\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores1, attention = scaled_dot_product(Q1, K1, V1)\n",
    "print(scores1)\n",
    "print(attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all functions to compute attention for ecah heads\n",
    "\n",
    "def attention(embed, WQ, WK, WV,):\n",
    "    embed_WK = np.dot(embed, WK)\n",
    "    embed_WV = np.dot(embed, WV)\n",
    "    embed_WQ = np.dot(embed, WQ)\n",
    "    scores = np.dot(embed_WQ, embed_WK.T)\n",
    "    # scores = scores / np.sqrt(embed_WK.shape[1])\n",
    "\n",
    "    # implement a temporary hack to scale the scores by 30\n",
    "    scores = scores /30\n",
    "\n",
    "    scores = softmax(scores)\n",
    "    attention = scores @ embed_WV\n",
    "    return scores, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.22405512</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.77594488</span><span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.16894812</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.83105188</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0.22405512\u001b[0m \u001b[1;36m0.77594488\u001b[0m\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m0.16894812\u001b[0m \u001b[1;36m0.83105188\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.55227775</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.20482485</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.20560079</span><span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.66251932</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.3614098</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.36224085</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m7.55227775\u001b[0m \u001b[1;36m8.20482485\u001b[0m \u001b[1;36m6.20560079\u001b[0m\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m7.66251932\u001b[0m \u001b[1;36m8.3614098\u001b[0m  \u001b[1;36m6.36224085\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.13475126</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.86524874</span><span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07080973</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.92919027</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0.13475126\u001b[0m \u001b[1;36m0.86524874\u001b[0m\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m0.07080973\u001b[0m \u001b[1;36m0.92919027\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.45857919</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.86481615</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.73093014</span><span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.6402672</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.92872572</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.85884518</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8.45857919\u001b[0m \u001b[1;36m3.86481615\u001b[0m \u001b[1;36m7.73093014\u001b[0m\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m8.6402672\u001b[0m  \u001b[1;36m3.92872572\u001b[0m \u001b[1;36m7.85884518\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores1, attention1 = attention(encoder_input, WQ1, WK1, WV1)\n",
    "scores2, attention2 = attention(encoder_input, WQ2, WK2, WV2)\n",
    "print(scores1)\n",
    "print(attention1)\n",
    "print(scores2)\n",
    "print(attention2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next layer of the encoder expects a one matrix. So we will need to concatenate the attentions for each heads horixontally or along the columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.55227775</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.20482485</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.20560079</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.45857919</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.86481615</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.73093014</span><span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.66251932</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.3614098</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.36224085</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.6402672</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.92872572</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.85884518</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m7.55227775\u001b[0m \u001b[1;36m8.20482485\u001b[0m \u001b[1;36m6.20560079\u001b[0m \u001b[1;36m8.45857919\u001b[0m \u001b[1;36m3.86481615\u001b[0m \u001b[1;36m7.73093014\u001b[0m\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m7.66251932\u001b[0m \u001b[1;36m8.3614098\u001b[0m  \u001b[1;36m6.36224085\u001b[0m \u001b[1;36m8.6402672\u001b[0m  \u001b[1;36m3.92872572\u001b[0m \u001b[1;36m7.85884518\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m6\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multi_head_output = np.concatenate((attention1, attention2), axis=1)\n",
    "print(multi_head_output)\n",
    "print(multi_head_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the concatenated weight matrix to thensame dimension as the encoder_input dimention by multiplying it with anther learned weight matrix of suitable dimention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m6\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W = np.array(\n",
    "    [\n",
    "        [0.79445237, 0.1081456, 0.27411536, 0.78394531],\n",
    "        [0.29081936, -0.36187258, -0.32312791, -0.48530339],\n",
    "        [-0.36702934, -0.76471963, -0.88058366, -1.73713022],\n",
    "        [-0.02305587, -0.64315981, -0.68306653, -1.25393866],\n",
    "        [0.29077448, -0.04121674, 0.01509932, 0.13149906],\n",
    "        [0.57451867, -0.08895355, 0.02190485, 0.24535932],\n",
    "    ]\n",
    ")\n",
    "print(W.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.47874302</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-13.18510726</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-11.5956346</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-17.04265726</span><span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.642255</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-13.48050171</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-11.87428539</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-17.49236571</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m11.47874302\u001b[0m \u001b[1;36m-13.18510726\u001b[0m \u001b[1;36m-11.5956346\u001b[0m  \u001b[1;36m-17.04265726\u001b[0m\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m \u001b[1;36m11.642255\u001b[0m   \u001b[1;36m-13.48050171\u001b[0m \u001b[1;36m-11.87428539\u001b[0m \u001b[1;36m-17.49236571\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attenton_layer_output = multi_head_output @ W\n",
    "print(attenton_layer_output)\n",
    "print(attenton_layer_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFN\n",
    "\n",
    "The attention layer output is then feed into a simple Feed Forward Neural Network of linear layer, RELU layer, and linear layer in that order. The first linear layer expands the embedding dimension ($dmodel$) to allow the model to learn more complex patterns, the RELU layer allow the model to learn non-linear function, and the final linear layer reverts the dimension back to the original dimension. That is it revrets the effect of the first linear layer so that we get back to the initial attenion layer dimension.\n",
    "\n",
    "\n",
    "$$FFN = RELU(xW_1 + b_1)W_2 + b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| W1.shape: (4, 8), W2.shape: (8, 4), b1.shape: (8,), b2.shape: (4,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4, 8), (8, 4), (8,), (4,))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create random weight for the FFN layers\n",
    "W1 = np.random.randn(4, 8)\n",
    "W2 = np.random.randn(8, 4)\n",
    "b1 = np.random.randn(8)\n",
    "b2 = np.random.randn(4)\n",
    "ic(W1.shape, W2.shape, b1.shape, b2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Forward pass function\n",
    "\n",
    "\n",
    "# def relu(x):\n",
    "#     return np.maximum(0, x)\n",
    "\n",
    "\n",
    "# def feed_forward(x, W1, b1, W2, b2):\n",
    "#     l1 = x @ W1 + b1 # linear layer 1\n",
    "#     l2 = relu(l1) # Non Linear layer 2\n",
    "#     l3 = l2 @ W2 + b2 # Linear layer 3\n",
    "#     return l3\n",
    "# output_encoder = feed_forward(attenton_layer_output, W1, b1, W2, b2)\n",
    "# output_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all functions to compute attention for ecah heads\n",
    "d_value = d_key = d_query = 3\n",
    "d_model = 4\n",
    "n_heads = 2\n",
    "d_feed_forward = 8\n",
    "\n",
    "\n",
    "def layer_norm(x, epsilon=1e-6):\n",
    "    mean = np.mean(x, axis=1, keepdims=True)\n",
    "    std = np.std(x, axis=1, keepdims=True)\n",
    "    return (x - mean) / (std + epsilon)\n",
    "\n",
    "\n",
    "def attention(\n",
    "    embed,\n",
    "    WQ,\n",
    "    WK,\n",
    "    WV,\n",
    "):\n",
    "    embed_WK = np.dot(embed, WK)\n",
    "    embed_WV = np.dot(embed, WV)\n",
    "    embed_WQ = np.dot(embed, WQ)\n",
    "    scores = np.dot(embed_WQ, embed_WK.T)\n",
    "    # scores = scores / np.sqrt(embed_WK.shape[1])\n",
    "\n",
    "    # implement a temporary hack to scale the scores by 30\n",
    "    scores = scores / 30\n",
    "\n",
    "    scores = softmax(scores)\n",
    "    attention = scores @ embed_WV\n",
    "    return attention\n",
    "\n",
    "\n",
    "def multi_attention_head(\n",
    "    embed,\n",
    "    WQs,\n",
    "    WKs,\n",
    "    WVs,\n",
    "):\n",
    "    attentions = np.concatenate(\n",
    "        [attention(embed, WQ, WK, WV) for WQ, WK, WV in zip(WQs, WKs, WVs)], axis=1\n",
    "    )\n",
    "    W = np.random.randn(n_heads * d_value, d_model)\n",
    "    return attentions @ W\n",
    "\n",
    "\n",
    "def multi_attention_head_with_layer_norm(embed, WQs, WKs, WVs):\n",
    "    attentions = np.concatenate(\n",
    "        [attention(embed, WQ, WK, WV) for WQ, WK, WV in zip(WQs, WKs, WVs)], axis=1\n",
    "    )\n",
    "    W = np.random.randn(n_heads * d_value, d_model)\n",
    "    attentions = attentions @ W  # revert back to the input dimension\n",
    "    residual = embed + attentions\n",
    "    attention_layer_output = layer_norm(residual)\n",
    "    return attention_layer_output\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def feed_forward(x, W1, b1, W2, b2):\n",
    "    l1 = x @ W1 + b1  # linear layer 1\n",
    "    l2 = relu(l1)  # Non Linear layer 2\n",
    "    l3 = l2 @ W2 + b2  # Linear layer 3\n",
    "    return l3\n",
    "\n",
    "\n",
    "def encoder_block_with_norm(embed, WQs, WKs, WVs, W1, b1, W2, b2):\n",
    "    attentions = multi_attention_head_with_layer_norm(embed, WQs, WKs, WVs)\n",
    "    # ic(attentions.shape)\n",
    "    encoder_output = feed_forward(attentions, W1, b1, W2, b2)\n",
    "    encoder_output = layer_norm(encoder_output + attentions)\n",
    "    return encoder_output\n",
    "\n",
    "\n",
    "def encoder_block(embed, WQs, WKs, WVs, W1, b1, W2, b2):\n",
    "    attentions = multi_attention_head(embed, WQs, WKs, WVs)\n",
    "    # ic(attentions.shape)\n",
    "    encoder_output = feed_forward(attentions, W1, b1, W2, b2)\n",
    "    return encoder_output\n",
    "\n",
    "\n",
    "def random_encoder_block(x):\n",
    "    WQs = [np.random.randn(d_model, d_query) for _ in range(n_heads)]\n",
    "    WKs = [np.random.randn(d_model, d_key) for _ in range(n_heads)]\n",
    "    WVs = [np.random.randn(d_model, d_value) for _ in range(n_heads)]\n",
    "    W1 = np.random.randn(d_model, d_feed_forward)\n",
    "    b1 = np.random.randn(d_feed_forward)\n",
    "    W2 = np.random.randn(d_feed_forward, d_model)\n",
    "    b2 = np.random.randn(d_model)\n",
    "    # return encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2)\n",
    "    return encoder_block_with_norm(x, WQs, WKs, WVs, W1, b1, W2, b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.59996341,  0.2418922 ,  0.19975211,  1.1583191 ],\n",
       "       [-1.59150373,  0.24296948,  0.17486338,  1.17367087]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_encoder_block(encoder_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x, n=6):\n",
    "    for _ in range(n):\n",
    "        x = random_encoder_block(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.91096696, -0.48395323,  0.97912822, -1.40614196],\n",
       "       [ 0.91096807, -0.48395335,  0.9791272 , -1.40614192]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(encoder_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six encoder blocks was used in the original paper. The below function simulate this implementation. Note that we got NaNs in our result. This is bacause small changes small chnages in ealier layers got amplified in later layers. Note that we are having to do a lot of multiplication as we go from one layer to another. This may lead to a very large value by the time we reach the last layer. This phenomenon is called gradient explosio in deep learning. There are two approaches to avoiding this issue:\n",
    "\n",
    "1. Residual connection\n",
    "2. Layer normalization\n",
    "\n",
    "**Residual Connection**\n",
    "To perform residual connection you add the input to the output as shown below.\n",
    "$Residual Connection = input + Layer(input)$\n",
    "\n",
    "We  will apply this to the output of the attention layer and the output of the feedforward layer. In this way we can prevent varnishing gradient. This  also have the side effect of preventing the other layesr in the stack to get waht was learned from the initial input.\n",
    "\n",
    "**Layer Normalization**\n",
    "We want to normalize the input along the embedding dimenssion, ensuring that each embedding will not be affceted by other samples in the batch. The embedding will have a mean of zero and std of 1. This helps with the flow of gradients. \n",
    "$$\\text{Layer Norm} = \\frac{(x - \\mu)}{\\sqrt{\\sigma^2 + \\epsilon}} * \\gamma + \\beta$$\n",
    "\n",
    "\n",
    "- $\\mu$ is the mean of the embedding\n",
    "- $\\sigma$ is the standard deviation of the embedding\n",
    "- $\\epsilon$ is a small number added to prevent zero division\n",
    "- $\\gamma$ and $\\beta$ are learned parameters that control both the scaling and shifting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets apply residual connection and layer normalization to each of the encoder block to avoid the problem stated previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.97333043</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.88099948</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1120454</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.01971444</span><span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.97332582</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.88100111</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.11204384</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.01971913</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.97333043\u001b[0m  \u001b[1;36m0.88099948\u001b[0m  \u001b[1;36m1.1120454\u001b[0m  \u001b[1;36m-1.01971444\u001b[0m\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m-0.97332582\u001b[0m  \u001b[1;36m0.88100111\u001b[0m  \u001b[1;36m1.11204384\u001b[0m \u001b[1;36m-1.01971913\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The output of a stack of six encoder blocks that captures the meaning of the input\n",
    "# sequence. This outout is then passed to the decoder.\n",
    "encoder_output = encoder(encoder_input)\n",
    "print(encoder_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Block\n",
    "\n",
    "The decoder block has two self-attention layers and a feed-forward layer as well. The decoder block takes two inputs:\n",
    "\n",
    "- Econder output\n",
    "- and a sequence of output from the decoder\n",
    "\n",
    "The decoder will start from a special start of sequence token and ends with end of sequence token. Each passes through the decoder blocks creates an output which is added to the original input to become the next input. \n",
    "\n",
    "Step 1: Decoder predics special SOS token\n",
    "Step 2: Decoder predicts the next token \"We\"\n",
    "Step 3: the next input is SOS + We\n",
    "Step 4: The decoder predicst \"are\"\n",
    "Step 5: The next input is SOS + We + are\n",
    "step 6: The decoder predicts \"good\"\n",
    "Step 7: The next input becomes SOS + We + are + good\n",
    "step 8: The decoder predicts \".\"\n",
    "Step 9: The next input becomes SOS + We + are + good.\n",
    "step 10: The decoder predicts \"EOS\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| decoder_input_emb.shape: (1, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m. \u001b[1;36m1\u001b[0m. \u001b[1;36m0\u001b[0m. \u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoder_input_emb = np.array([1, 0, 0, 0], dtype=\"float\").reshape(1, -1)\n",
    "decoder_positional_emb = position_embedding(decoder_input_emb)\n",
    "decoder_input_emb = decoder_input_emb + decoder_positional_emb\n",
    "ic(decoder_input_emb.shape)\n",
    "print(decoder_input_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.28566176,  1.24780425,  0.49322824, -1.45537072]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 4\n",
    "n_heads = 2\n",
    "\n",
    "WQs = [np.random.randn(d_model, d_query) for _ in range(n_heads)]\n",
    "WKs = [np.random.randn(d_model, d_key) for _ in range(n_heads)]\n",
    "WVs = [np.random.randn(d_model, d_value) for _ in range(n_heads)]\n",
    "\n",
    "Z_self_attention = multi_attention_head_with_layer_norm(decoder_input_emb, WQs, WKs, WVs)\n",
    "Z_self_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder_attention(encoder_output, attention_input, WQ, WK, WV):\n",
    "    # The next three lines are the key difference!\n",
    "    K = encoder_output @ WK  # Note that now we pass the previous encoder output!\n",
    "    V = encoder_output @ WV  # Note that now we pass the previous encoder output!\n",
    "    Q = attention_input @ WQ  # Same as self-attention\n",
    "\n",
    "    # This stays the same\n",
    "    scores = Q @ K.T\n",
    "    scores = scores / np.sqrt(d_key)\n",
    "    scores = softmax(scores)\n",
    "    scores = scores @ V\n",
    "    return scores\n",
    "\n",
    "\n",
    "def multi_head_encoder_decoder_attention(\n",
    "    encoder_output, attention_input, WQs, WKs, WVs\n",
    "):\n",
    "    # Note that now we pass the previous encoder output!\n",
    "    attentions = np.concatenate(\n",
    "        [\n",
    "            encoder_decoder_attention(encoder_output, attention_input, WQ, WK, WV)\n",
    "            for WQ, WK, WV in zip(WQs, WKs, WVs)\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    W = np.random.randn(n_heads * d_value, d_model)\n",
    "    attentions = attentions @ W  # revert back to the input dimension\n",
    "    residual = attention_input + attentions\n",
    "    attention_layer_output = layer_norm(residual)\n",
    "    return attention_layer_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.56416499</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.21883807</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.89831987</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.68132293</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.56416499\u001b[0m  \u001b[1;36m0.21883807\u001b[0m  \u001b[1;36m0.89831987\u001b[0m \u001b[1;36m-1.68132293\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WQs = [np.random.randn(d_model, d_query) for _ in range(n_heads)]\n",
    "WKs = [np.random.randn(d_model, d_key) for _ in range(n_heads)]\n",
    "WVs = [np.random.randn(d_model, d_value) for _ in range(n_heads)]\n",
    "Z_encoder_decoder = multi_head_encoder_decoder_attention(\n",
    "    encoder_output, Z_self_attention, WQs, WKs, WVs\n",
    ")\n",
    "print(Z_encoder_decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(decoder_input, encoder_output,WQs_self_attention, WKs_self_attention, WVs_self_attention,\n",
    "    WQs_ed_attention, WKs_ed_attention, WVs_ed_attention,\n",
    "    W1, b1, W2, b2):\n",
    "    Z = multi_attention_head_with_layer_norm(decoder_input,\n",
    "                         WQs_self_attention, WKs_self_attention, WVs_self_attention)\n",
    "\n",
    "    Z_encoder_decoder = multi_head_encoder_decoder_attention( encoder_output, Z, WQs_ed_attention, WKs_ed_attention, WVs_ed_attention)\n",
    "    output = feed_forward(Z_encoder_decoder, W1, b1, W2, b2)\n",
    "    output = layer_norm(output + Z_encoder_decoder)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_decoder_block(x, encoder_output):\n",
    "    # Just a bunch of random initializations\n",
    "    WQs_self_attention = [\n",
    "        np.random.randn(d_model, d_query) for _ in range(n_heads)\n",
    "    ]\n",
    "    WKs_self_attention = [\n",
    "        np.random.randn(d_model, d_key) for _ in range(n_heads)\n",
    "    ]\n",
    "    WVs_self_attention = [\n",
    "        np.random.randn(d_model, d_value) for _ in range(n_heads)\n",
    "    ]\n",
    "\n",
    "    WQs_ed_attention = [\n",
    "        np.random.randn(d_model, d_query) for _ in range(n_heads)\n",
    "    ]\n",
    "    WKs_ed_attention = [\n",
    "        np.random.randn(d_model, d_key) for _ in range(n_heads)\n",
    "    ]\n",
    "    WVs_ed_attention = [\n",
    "        np.random.randn(d_model, d_value) for _ in range(n_heads)\n",
    "    ]\n",
    "\n",
    "    W1 = np.random.randn(d_model, d_feed_forward)\n",
    "    b1 = np.random.randn(d_feed_forward)\n",
    "    W2 = np.random.randn(d_feed_forward, d_model)\n",
    "    b2 = np.random.randn(d_model)\n",
    "\n",
    "    return decoder_block(\n",
    "        x,\n",
    "        encoder_output,\n",
    "        WQs_self_attention,\n",
    "        WKs_self_attention,\n",
    "        WVs_self_attention,\n",
    "        WQs_ed_attention,\n",
    "        WKs_ed_attention,\n",
    "        WVs_ed_attention,\n",
    "        W1,\n",
    "        b1,\n",
    "        W2,\n",
    "        b2,\n",
    "    )\n",
    "\n",
    "\n",
    "def decoder(x, decoder_embedding, n=6):\n",
    "    for _ in range(n):\n",
    "        x = random_decoder_block(x, decoder_embedding)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0081298 , -1.62301003,  0.68241529,  0.94872453]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_encoder_output = decoder(decoder_input_emb, encoder_output)\n",
    "decoder_encoder_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the encoder-decoder block. We do need to transfrm the decoder output into a vector of vocab size where each elemets of the vocab is the probailitity of the word at that index being the next word. \n",
    "We accomplish this with a linear layer whose weight is of the d_model by vocab_size.\n",
    "We then apply softmax to the matrix multiplication of the decoder output and linear weight to get the probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.41450158e-03</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.85692061e-01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.79976932e-03</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.58459044e-03</span>\n",
       "  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.10519641e-02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.24151978e-01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.99984194e-01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.16536424e-05</span>\n",
       "  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.38906110e-02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.93786767e-02</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m3.41450158e-03\u001b[0m \u001b[1;36m2.85692061e-01\u001b[0m \u001b[1;36m5.79976932e-03\u001b[0m \u001b[1;36m6.58459044e-03\u001b[0m\n",
       "  \u001b[1;36m1.10519641e-02\u001b[0m \u001b[1;36m1.24151978e-01\u001b[0m \u001b[1;36m4.99984194e-01\u001b[0m \u001b[1;36m5.16536424e-05\u001b[0m\n",
       "  \u001b[1;36m1.38906110e-02\u001b[0m \u001b[1;36m4.93786767e-02\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def linear(x, W, b):\n",
    "    output = np.dot(x, W) + b\n",
    "    output = softmax(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "W_linear = np.random.randn(4, 10)  # vocab size is 10; dmodel is 4\n",
    "b = np.random.randn(10)\n",
    "\n",
    "final_output = linear(decoder_encoder_output, W_linear, b)\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets write the encoder-decoder to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'hello'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.53796188</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.24626629</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.49815192</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.60167653</span><span style=\"font-weight: bold\">]])</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'mundo'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.45458834</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.47166079</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.90736952</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.62441071</span><span style=\"font-weight: bold\">]])</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'world'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.37168928</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.28763343</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.29041929</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.00904514</span><span style=\"font-weight: bold\">]])</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'how'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.3501096</span> , <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.57190853</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.10699653</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.203589</span>  <span style=\"font-weight: bold\">]])</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'?'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.96590256</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.05765577</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.60619752</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.5982328</span> <span style=\"font-weight: bold\">]])</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'EOS'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.11630126</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.92822043</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.5726166</span> , <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3643059</span> <span style=\"font-weight: bold\">]])</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'SOS'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.03016372</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.40142813</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.95749328</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.26818021</span><span style=\"font-weight: bold\">]])</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.37256056</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08240517</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.81912088</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.17817087</span><span style=\"font-weight: bold\">]])</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'hola'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.75913399</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4892894</span> , <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.60227044</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.78062945</span><span style=\"font-weight: bold\">]])</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'c'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.53080636</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.35450948</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.13119078</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.54694581</span><span style=\"font-weight: bold\">]])</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'hello'\u001b[0m: \u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.53796188\u001b[0m, \u001b[1;36m-1.24626629\u001b[0m,  \u001b[1;36m0.49815192\u001b[0m,  \u001b[1;36m0.60167653\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'mundo'\u001b[0m: \u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.45458834\u001b[0m,  \u001b[1;36m1.47166079\u001b[0m, \u001b[1;36m-0.90736952\u001b[0m,  \u001b[1;36m0.62441071\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'world'\u001b[0m: \u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.37168928\u001b[0m,  \u001b[1;36m0.28763343\u001b[0m,  \u001b[1;36m0.29041929\u001b[0m, \u001b[1;36m-0.00904514\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'how'\u001b[0m: \u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-1.3501096\u001b[0m , \u001b[1;36m-1.57190853\u001b[0m, \u001b[1;36m-0.10699653\u001b[0m, \u001b[1;36m-0.203589\u001b[0m  \u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'?'\u001b[0m: \u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.96590256\u001b[0m, \u001b[1;36m-1.05765577\u001b[0m,  \u001b[1;36m0.60619752\u001b[0m, \u001b[1;36m-0.5982328\u001b[0m \u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'EOS'\u001b[0m: \u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.11630126\u001b[0m,  \u001b[1;36m0.92822043\u001b[0m, \u001b[1;36m-0.5726166\u001b[0m , \u001b[1;36m-0.3643059\u001b[0m \u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'SOS'\u001b[0m: \u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-2.03016372\u001b[0m,  \u001b[1;36m1.40142813\u001b[0m,  \u001b[1;36m0.95749328\u001b[0m,  \u001b[1;36m1.26818021\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'a'\u001b[0m: \u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.37256056\u001b[0m,  \u001b[1;36m0.08240517\u001b[0m,  \u001b[1;36m0.81912088\u001b[0m, \u001b[1;36m-2.17817087\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'hola'\u001b[0m: \u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.75913399\u001b[0m,  \u001b[1;36m0.4892894\u001b[0m , \u001b[1;36m-0.60227044\u001b[0m, \u001b[1;36m-1.78062945\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'c'\u001b[0m: \u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1.53080636\u001b[0m,  \u001b[1;36m1.35450948\u001b[0m,  \u001b[1;36m0.13119078\u001b[0m, \u001b[1;36m-0.54694581\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocabulary = [\n",
    "    \"hello\",\n",
    "    \"mundo\",\n",
    "    \"world\",\n",
    "    \"how\",\n",
    "    \"?\",\n",
    "    \"EOS\",\n",
    "    \"SOS\",\n",
    "    \"a\",\n",
    "    \"hola\",\n",
    "    \"c\",\n",
    "]\n",
    "embedding_reps = np.random.randn(10, 1, 4)\n",
    "vocabulary_embeddings = {word: embedding_reps[i] for i, word in enumerate(vocabulary)}\n",
    "print(vocabulary_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.53796188, -1.24626629,  0.49815192,  0.60167653])]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocabulary_embeddings['hello'][0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'World']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hello World\"\n",
    "text.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.53796188</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.24626629</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.49815192</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.60167653</span><span style=\"font-weight: bold\">])</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.37168928</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.28763343</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.29041929</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.00904514</span><span style=\"font-weight: bold\">])</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.53796188\u001b[0m, \u001b[1;36m-1.24626629\u001b[0m,  \u001b[1;36m0.49815192\u001b[0m,  \u001b[1;36m0.60167653\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.37168928\u001b[0m,  \u001b[1;36m0.28763343\u001b[0m,  \u001b[1;36m0.29041929\u001b[0m, \u001b[1;36m-0.00904514\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.53796188</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.24626629</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.49815192</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.60167653</span><span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.37168928</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.28763343</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.29041929</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.00904514</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.53796188\u001b[0m \u001b[1;36m-1.24626629\u001b[0m  \u001b[1;36m0.49815192\u001b[0m  \u001b[1;36m0.60167653\u001b[0m\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m-0.37168928\u001b[0m  \u001b[1;36m0.28763343\u001b[0m  \u001b[1;36m0.29041929\u001b[0m \u001b[1;36m-0.00904514\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"hello world\"\n",
    "xx = [vocabulary_embeddings[token][0] for token in text.split()]\n",
    "print(xx)\n",
    "print(np.array(xx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Embedding representation of the encoder input <span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.53796188</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.24626629</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.49815192</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.60167653</span><span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.37168928</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.28763343</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.29041929</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.00904514</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Embedding representation of the encoder input \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.53796188\u001b[0m \u001b[1;36m-1.24626629\u001b[0m  \u001b[1;36m0.49815192\u001b[0m  \u001b[1;36m0.60167653\u001b[0m\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m-0.37168928\u001b[0m  \u001b[1;36m0.28763343\u001b[0m  \u001b[1;36m0.29041929\u001b[0m \u001b[1;36m-0.00904514\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Embedding generated by the encoder <span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.5232174</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.19409875</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.15148978</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.48060842</span><span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.52322285</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1940391</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.15153979</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.48072354</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Embedding generated by the encoder \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-1.5232174\u001b[0m   \u001b[1;36m1.19409875\u001b[0m \u001b[1;36m-0.15148978\u001b[0m  \u001b[1;36m0.48060842\u001b[0m\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m\u001b[1;36m-1.52322285\u001b[0m  \u001b[1;36m1.1940391\u001b[0m  \u001b[1;36m-0.15153979\u001b[0m  \u001b[1;36m0.48072354\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Iteration <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> next token c with probability of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2186958354424255</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Iteration \u001b[1;36m0\u001b[0m next token c with probability of \u001b[1;36m0.2186958354424255\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Iteration <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> next token EOS with probability of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.18025941106539678</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Iteration \u001b[1;36m1\u001b[0m next token EOS with probability of \u001b[1;36m0.18025941106539678\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'SOS c EOS'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(input_sequence, max_iters=10):\n",
    "    # Encode the input sequence into embeddings\n",
    "    # We skip the positional encoding step\n",
    "    embedding_inputs = [vocabulary_embeddings[token][0] for token in input_sequence]\n",
    "    embedding_inputs = np.array(embedding_inputs)\n",
    "    print(\"Embedding representation of the encoder input\", embedding_inputs)\n",
    "\n",
    "    # lets generate the encoder outout using the encoder block options\n",
    "    encoder_output = encoder(embedding_inputs)\n",
    "    print(\"Embedding generated by the encoder\", encoder_output)\n",
    "\n",
    "    # We initialize the first output of the decoder block with SOS token\n",
    "    output = \"SOS\"\n",
    "    sequence = vocabulary_embeddings[\"SOS\"]\n",
    "\n",
    "    # Create the random matrices for the linear layer\n",
    "    W_linear = np.random.randn(d_model, len(vocabulary))\n",
    "    b_linear = np.random.randn(len(vocabulary))\n",
    "\n",
    "    # lets limit the number of decoding runs to avoid decoding\n",
    "    # for too long without hitting the \"EOS\" token\n",
    "    for i in range(max_iters):\n",
    "        # Decoder step\n",
    "        decoder_output = decoder(sequence, encoder_output)\n",
    "        logits = linear(decoder_output, W_linear, b_linear)\n",
    "        probs = softmax(logits)\n",
    "\n",
    "        # get the most like;y token - greedy smapling\n",
    "        next_token = vocabulary[np.argmax(probs)]\n",
    "        output += \" \" + next_token\n",
    "        print(\n",
    "            \"Iteration\",\n",
    "            i,\n",
    "            \"next token\",\n",
    "            next_token,\n",
    "            \"with probability of\",\n",
    "            np.max(probs),\n",
    "        )\n",
    "        # If the next token is the end token, we return the sequence\n",
    "        if next_token == \"EOS\":\n",
    "            return output\n",
    "    return output\n",
    "\n",
    "\n",
    "generate([\"hello\", \"world\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
